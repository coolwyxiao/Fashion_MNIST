{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion_MNIST",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coolwyxiao/Fashion_MNIST/blob/main/Fashion_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z1LaTE4NJ8Q"
      },
      "source": [
        "Show Fashion MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmjtXLzPDSXf"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.sparse as sparse\n",
        "import random\n",
        "\n",
        "train = np.load(\"train.npy\") #(60000,28,28)\n",
        "train = np.array(train)\n",
        "train_label = np.load(\"train_labels.npy\")\n",
        "train_label = list(train_label)\n",
        "\n",
        "test = np.load(\"test.npy\") #(10000,28,28)\n",
        "test = np.array(test)\n",
        "test_label = np.load(\"test_labels.npy\")\n",
        "test_label = list(test_label)\n",
        "\n",
        "def get_index(label):\n",
        "  '''\n",
        "  input: label array from train_labels.npy\n",
        "  return: list of indexes which meet requirements of problem\n",
        "  '''\n",
        "  index_1 = [] # extract indexes with {2,5}\n",
        "  index_2 = [] # extract indexes with {2,5,7}\n",
        "  value_1 = []\n",
        "  value_2 = []\n",
        "  value_binary = []\n",
        "  cnn_binary = []\n",
        "  j = 0\n",
        "  for i in label:\n",
        "      if i == 2 or i == 5 or i==7:\n",
        "        index_2.append(j)\n",
        "        value_2.append(i)\n",
        "        if i == 2:\n",
        "          cnn_binary.append(0)\n",
        "        if i == 5:\n",
        "          cnn_binary.append(1)\n",
        "        else:\n",
        "          cnn_binary.append(2)\n",
        "      if i == 2 or i == 5:\n",
        "        index_1.append(j)\n",
        "        value_1.append(i)\n",
        "        if i == 2:\n",
        "          value_binary.append(-1)\n",
        "        else:\n",
        "          value_binary.append(1)\n",
        "      j += 1\n",
        "  return(index_1,index_2,value_1,value_2,value_binary,cnn_binary)\n",
        "\n",
        "index = get_index(train_label)\n",
        "index_1 = index[0] # list of indexes with data associated with {2,5}\n",
        "index_2 = index[1] # list of indexes with data associated with {2,5,7}\n",
        "value_1 = index[2] # list of data labels associated with {2,5}\n",
        "value_2 = index[3] # list of data labels associated with {2,5,7}\n",
        "value_binary_1 = np.array(index[4]) # array of data associated with {2,5}, convert value to -1 and 1\n",
        "cnn_binary_1 = np.array(index[5]) # array of data associated with {2,5,7}, convert value to 0,1,2\n",
        "data_1 = train[np.array(index_1)] # array of data be selected with {2,5}(12000 items)\n",
        "data_2 = train[np.array(index_2)] # array of data be selected with {2,5,7}(18000 items)\n",
        "\n",
        "index_test = get_index(test_label)\n",
        "index_3 = index_test[0] # list of indexes with data associated with {2,5}\n",
        "index_4 = index_test[1] # list of indexes with data associated with {2,5,7}\n",
        "value_3 = index_test[2] # list of data labels associated with {2,5}\n",
        "value_4 = index_test[3] # list of data labels associated with {2,5,7}\n",
        "value_binary_2 = np.array(index_test[4]) # array of data associated with {2,5}, convert value to -1 and 1\n",
        "cnn_binary_2 = np.array(index_test[5]) # array of data associated with {2,5,7}, convert value to 0,1,2\n",
        "data_3 = test[np.array(index_3)] # array of data be selected with {2,5} (2000 items)\n",
        "data_4 = test[np.array(index_4)] # array of data be selected with {2,5,7} (3000 items)\n",
        "\n",
        "#plt.spy(train[0],markersize = 4)\n",
        "\n",
        "def random_image(index, data):\n",
        "  '''\n",
        "  input: list of indexes we need to use, data array we select\n",
        "  '''\n",
        "  length = len(data)\n",
        "  for x in range(10):\n",
        "      plt.figure(x)\n",
        "      ran =  random.randint(0,length)\n",
        "      val = index[ran]\n",
        "      value = train_label[val]\n",
        "      plt.imshow(data[ran])\n",
        "      print(value)\n",
        "\n",
        "random_image(index_2,data_2) # answer of question 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwOIfFO8NR5r"
      },
      "source": [
        "1. Pegasos method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9IBLwveXXqp"
      },
      "source": [
        "def convert_matrix(x):\n",
        "    '''convert matrix to 1D vector'''\n",
        "    pixel_length = len(x[0, 0, :])\n",
        "    num_data = len(x[:, 0, 0])      # for the training set\n",
        "    x = np.reshape(x, (num_data, pixel_length * pixel_length, 1))       # reshape to (12000, 28*28, 1)(for Training Set)\n",
        "    return x\n",
        "    \n",
        "def pegasos(x, y, m, n, weights=None, iterations=2000, lam=1, b=32): # lam=100, b=10\n",
        "    '''\n",
        "    input: x=data_1(array), y=value_binary_1(array), b is mini-batch size, m and n are test data(data_3) and test labels(value_binary_1(array))\n",
        "    '''\n",
        "    x = convert_matrix(x)\n",
        "    m = convert_matrix(m)\n",
        "    error_list_1 = []\n",
        "    error_list_2 = []\n",
        "    if type(weights) == type(None): weights = np.zeros((len(x[0]),1)) \n",
        "    num_S = len(y)\n",
        "    for i in range(iterations):\n",
        "      step = 1/(lam*(i+1))\n",
        "      gradient = np.zeros((len(x[0]),1))\n",
        "      j = 0\n",
        "      while j < b:\n",
        "        it = random.randint(0, num_S-1)\n",
        "        decision = y[it] * np.dot(weights.T, x[it])[0, 0]\n",
        "        if decision < 1:\n",
        "           gradient += y[it] * x[it]\n",
        "        j += 1\n",
        "      value_1 = lam * weights - gradient/b \n",
        "      weights_1 = weights - step * value_1\n",
        "      weights = min(1, lam**(-0.5)/np.linalg.norm(weights_1)) * weights_1 \n",
        "      error_list_1.append(get_errors(x, y, weights))\n",
        "      error_list_2.append(get_errors(m, n, weights))\n",
        "      if i % 100 == 0:\n",
        "        print(i/iterations)\n",
        "    return error_list_1, error_list_2\n",
        "\n",
        "\n",
        "def get_errors(x, y, weights):\n",
        "    '''\n",
        "    input: x=data_1(array), y=value_binary_1(array), weights is a matrix with row x and column y\n",
        "    '''\n",
        "    a = 0\n",
        "    i = 0\n",
        "    pixel_length = len(x[0, 0, :])\n",
        "    num_data = len(x[:, 0, 0])\n",
        "    for i in range(num_data):\n",
        "      if (np.dot(weights.T, x[i])[0, 0] < 0):\n",
        "        print(np.dot(weights.T, x[i])[0, 0])\n",
        "      decision = y[i] * (np.dot(weights.T, x[i])[0, 0])\n",
        "      if decision < 1:\n",
        "        a += 1\n",
        "    error = a/len(x)\n",
        "    return error\n",
        "\n",
        "error_list_train, error_list_test = (pegasos(data_1,value_binary_1, data_3, value_binary_2))\n",
        "print(error_list_train)\n",
        "print(error_list_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRBGR3NwNYfW"
      },
      "source": [
        "Visualize Categorization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XFEhz1NnSh1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(0, dpi = 130)\n",
        "plt.plot(range(2000), error_list_train, color=\"blue\", label=\"Train Error\")\n",
        "plt.legend()\n",
        "plt.figure(1, dpi = 130)\n",
        "plt.plot(range(2000), error_list_test, color=\"red\", label=\"Test Error\")\n",
        "plt.legend(loc='upper right')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkJv2DCVNiDv"
      },
      "source": [
        "2. Adagrad Method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7BknE1P7Kz0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def print_error(error_list_train, error_list_test, total_iteration):\n",
        "  plt.figure(0, dpi = 130)\n",
        "  plt.plot(range(total_iteration), error_list_train, color=\"blue\", label=\"Train Error\")\n",
        "  plt.legend()\n",
        "  plt.figure(1, dpi = 130)\n",
        "  plt.plot(range(total_iteration), error_list_test, color=\"red\", label=\"Test Error\")\n",
        "  plt.legend(loc='upper right')\n",
        "\n",
        "def convert_matrix(x):\n",
        "    '''convert matrix to 1D vector'''\n",
        "    pixel_length = len(x[0, 0, :])\n",
        "    num_data = len(x[:, 0, 0])      # for the training set\n",
        "    x = np.reshape(x, (num_data, pixel_length * pixel_length, 1))       # reshape to (12000, 28*28, 1)(for Training Set)\n",
        "    return x\n",
        "\n",
        "\n",
        "def adagrad(x, y, m, n, weights=None, iterations=1000, lam=1, b=32, eta=0.1):\n",
        "  '''\n",
        "  input: x=training data(data_1), y=training labels(value_binary_1),m and n are test data and labels\n",
        "  b is mini-batch size\n",
        "  '''\n",
        "  x = convert_matrix(x) # this function convert matrix to 1-D vector\n",
        "  m = convert_matrix(m)\n",
        "  error_list_1 = []\n",
        "  error_list_2 = []\n",
        "  if type(weights) == type(None): weights = np.zeros((len(x[0,:]),1)) \n",
        "  D = len(x[0,:]) # dimension D, which is a number\n",
        "  array_s = np.ones(D)\n",
        "  for i in range(iterations):\n",
        "    gradient = np.zeros((len(x[0,:]),1))\n",
        "    random = np.random.choice(x.shape[0],b,replace = False)\n",
        "    for j in random:\n",
        "      decision_1 = y[j] * np.dot(weights.T, x[j])[0,0]\n",
        "      if decision_1 < 1:\n",
        "        gradient += y[j] * x[j]\n",
        "    #step = step * np.linalg.inv(G)\n",
        "    gradient = lam * weights - gradient/b # same with Pegasos\n",
        "    array_s = array_s + np.dot(gradient.T, gradient) + 1e-10\n",
        "    # print(np.diag(np.sqrt(1/array_s)).shape) \n",
        "    weights = weights - eta * np.dot((np.diag(np.sqrt(1/array_s).flatten())),gradient)\n",
        "    weights = min(1, lam**(-0.5)/(np.linalg.norm(weights))) * weights\n",
        "    error_list_1.append(get_errors(x, y, weights)) # store error rate in each iteration in error_list\n",
        "    error_list_2.append(get_errors(m, n, weights))\n",
        "    if i % 10 == 0: \n",
        "        print(i/iterations)\n",
        "  return error_list_1, error_list_2\n",
        "\n",
        "def get_errors(x, y, weights):\n",
        "    '''\n",
        "    input: x=data_1(array), y=value_binary_1(array), weights is a matrix with row x and column y\n",
        "    '''\n",
        "    a = 0\n",
        "    i = 0\n",
        "    pixel_length = len(x[0, 0, :])\n",
        "    num_data = len(x[:, 0, 0])\n",
        "    for i in range(num_data):\n",
        "#       if (np.dot(weights.T, x[i])[0, 0] < 0):\n",
        "      decision = y[i] * (np.dot(weights.T, x[i])[0, 0])\n",
        "      if decision < 1:\n",
        "        a += 1\n",
        "    error = a/len(x)\n",
        "    return error\n",
        "\n",
        "error_list_train_1, error_list_test_1 = (adagrad(data_1,value_binary_1, data_3, value_binary_2))\n",
        "# print(error_list_train_1)\n",
        "# print(error_list_test_1)\n",
        "print_error(error_list_train_1, error_list_test_1, 1000)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWMRjZMrJ0Mn",
        "outputId": "19f7ab19-9568-472c-e974-d9886a4772d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(error_list_test_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5, 0.0865, 0.0865, 0.0285, 0.0245, 0.0245, 0.0245, 0.015, 0.0115, 0.0115, 0.0115, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.006, 0.006, 0.006, 0.0085, 0.0085, 0.0085, 0.0085, 0.0085, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.007, 0.007, 0.007, 0.007, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.0025, 0.0025, 0.0025, 0.0025, 0.0045, 0.0045, 0.0045, 0.0045, 0.0045, 0.0045, 0.0045, 0.0045, 0.0045, 0.0045, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0035, 0.0035, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.006, 0.006, 0.006, 0.006, 0.002, 0.002, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.002, 0.0035, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0015, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0045, 0.0045, 0.0045, 0.0045, 0.007, 0.007, 0.007, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.002, 0.002, 0.003, 0.003, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.001, 0.001, 0.001, 0.001, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGRle67vNoOH"
      },
      "source": [
        "Visualize Categorization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z78EuIUKoaeM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(0, dpi = 130)\n",
        "plt.plot(range(2000), error_list_train_1, color=\"blue\", label=\"Train Error\")\n",
        "plt.legend()\n",
        "plt.figure(1, dpi = 130)\n",
        "plt.plot(range(2000), error_list_test_1, color=\"red\", label=\"Test Error\")\n",
        "plt.legend(loc='upper right')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBKNG3eNGkhr",
        "outputId": "ccc95142-de88-4051-ea34-9ff4e91478fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "array_s = np.array([1,2,3,4])\n",
        "b=4\n",
        "G = np.diag(np.sqrt(array_s/b))\n",
        "G"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5       , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.70710678, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.8660254 , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQSn0rtiNwmV"
      },
      "source": [
        "3. Categorize Multiple Class Based on Pegasos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTh9m2QFERMA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def print_error(error_list_train, error_list_test, total_iteration):\n",
        "  plt.figure(0, dpi = 130)\n",
        "  plt.plot(range(total_iteration), error_list_train, color=\"blue\", label=\"Train Error\")\n",
        "  plt.legend()\n",
        "  plt.figure(1, dpi = 130)\n",
        "  plt.plot(range(total_iteration), error_list_test, color=\"red\", label=\"Test Error\")\n",
        "  plt.legend(loc='upper right')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq-iJCjmTh33"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def convert_matrix(x):\n",
        "    '''convert matrix to 1D vector'''\n",
        "    pixel_length = len(x[0, 0, :])\n",
        "    num_data = len(x[:, 0, 0])      # for the training set\n",
        "    x = np.reshape(x, (num_data, pixel_length * pixel_length, 1))       # reshape to (12000, 28*28, 1)(for Training Set)\n",
        "    return x\n",
        "\n",
        "def get_minibatch(batch_size, data_size): #  At=np.random.choice(len(X),self.b,replace=False)\n",
        "    '''\n",
        "    return a list of element inside a minibatch\n",
        "    '''\n",
        "    data_selected = []\n",
        "    for i in range(batch_size):\n",
        "      data_selected.append(random.randint(0, data_size-1))\n",
        "    return data_selected\n",
        "\n",
        "\n",
        "def multiclass_gradient(result, class_num, gradient, label, data):\n",
        "    \"\"\"\n",
        "    This function could return the result of gradient corresponding to the rule:\n",
        "    result: np.dot(weights[:, i].T, x[it])[0, 0] (which means the prediction of the current SVM)\n",
        "    class[0]: +1: 2/ -1: 5; \n",
        "    class[1]: +1: 2/ -1: 7;\n",
        "    class[2]: +1: 5/ -1: 7;\n",
        "    \"\"\"\n",
        "    if (class_num == 0):\n",
        "      # if label != 7 and prediction is wrong, change gradient, else: do nothing(don't change the gradient)\n",
        "      if (label == 2 and result < 1):\n",
        "        gradient[:, class_num:class_num+1] += data\n",
        "      elif (label == 5 and result > 1):\n",
        "        gradient[:, class_num:class_num+1] -= data\n",
        "\n",
        "    elif (class_num == 1):\n",
        "      # if label != 5 and prediction is wrong, change gradient, else: do nothing(don't change the gradient)\n",
        "      if (label == 2 and result < 1):\n",
        "        gradient[:, class_num:class_num+1] += data\n",
        "      elif (label == 7 and result > 1):\n",
        "        gradient[:, class_num:class_num+1] -= data\n",
        "\n",
        "    elif (class_num == 2):\n",
        "      # if label != 2 and prediction is wrong, change gradient, else: do nothing(don't change the gradient)\n",
        "      if (label == 5 and result < 1):\n",
        "        gradient[:, class_num:class_num+1] += data\n",
        "      elif (label == 7 and result > 1):\n",
        "        gradient[:, class_num:class_num+1] -= data\n",
        "        \n",
        "    return gradient\n",
        "\n",
        "\n",
        "def pegasos_multiclass(x, y, m, n, num_class = 3, weights=None, iterations=2000, lam=1, b=64):\n",
        "    '''\n",
        "    input: x=data_1(array), y=value_binary_1(array), b is mini-batch size, m and n are test data(data_3) and test labels(value_binary_1(array))\n",
        "    Chosen rule for multiclass: \n",
        "    class[0]: +1: 2/ -1: 5; \n",
        "    class[1]: +1: 2/ -1: 7;\n",
        "    class[2]: +1: 5/ -1: 7;\n",
        "    '''\n",
        "    x = convert_matrix(x)\n",
        "    m = convert_matrix(m)\n",
        "    error_list_1 = []\n",
        "    error_list_2 = []\n",
        "    if type(weights) == type(None): weights = np.zeros((len(x[0]),num_class)) # weights: row x, column y (28*28, 3)\n",
        "    num_S = len(y)\n",
        "    mini_batch = get_minibatch(b, num_S)  # a list of index of the data\n",
        "    for j in range(iterations):\n",
        "      step = 1/(lam*(j+1))\n",
        "      gradient = np.zeros((len(x[0]), num_class)) # shape: (28*28, 3) \n",
        "      for i in range(num_class):\n",
        "#         it = random.randint(0, num_S-1)\n",
        "        for it in range(len(mini_batch)):\n",
        "          prediction =  np.dot(weights[:, i:i+1].T, x[it])[0, 0] \n",
        "          gradient = multiclass_gradient(prediction, i, gradient, y[it], x[it])  # Need to get the gradient following the rules\n",
        "        value_1 = lam * weights[:, i:i+1] - gradient[:,i:i+1]/b # value_1: row x, column y\n",
        "        weights_1 = weights[:, i:i+1] - step * value_1\n",
        "        weights[:,i:i+1] = min(1, lam**(-0.5)/np.linalg.norm(weights_1)) * weights_1 # weights: row x, column y\n",
        "\n",
        "      error_list_1.append(get_errors_multiclass(x, y, weights, num_class))\n",
        "      error_list_2.append(get_errors_multiclass(m, n, weights, num_class))\n",
        "      if j % 100 == 0:\n",
        "        print(j/iterations*100, \"%\")\n",
        "        # print(\"weights[20:25,:] \")\n",
        "        # print(weights[20:25,:])\n",
        "        # print(decision)\n",
        "    return error_list_1, error_list_2, iterations\n",
        "\n",
        "def decision_multiclass(prediction, class_num):\n",
        "    \"\"\"\n",
        "    return the true decision using multiclass classification following the rules:\n",
        "    class[0]: +1: 2/ -1: 5; \n",
        "    class[1]: +1: 2/ -1: 7;\n",
        "    class[2]: +1: 5/ -1: 7;\n",
        "    \"\"\"\n",
        "    if class_num == 0:\n",
        "      result = 5 if prediction < 1 else 2\n",
        "    elif class_num == 1:\n",
        "      result = 7 if prediction < 1 else 2\n",
        "    elif class_num == 2:\n",
        "      result = 7 if prediction < 1 else 5\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_errors_multiclass(x, y, weights, num_class):\n",
        "    '''\n",
        "    input: x=data_1(array), y=value_binary_1(array), weights is a matrix with row x and column y\n",
        "    '''\n",
        "    a = 0\n",
        "#     i = 0\n",
        "    pixel_length = len(x[0, 0, :])\n",
        "    num_data = len(x[:, 0, 0])\n",
        "    for i in range(num_data):\n",
        "      decision = []\n",
        "      for j in range(num_class):                    # collect the result of prediction for vote process\n",
        "        prediction = (np.dot(weights[:, j:j+1].T, x[i])[0, 0])\n",
        "        decision.append(decision_multiclass(prediction, j))\n",
        "      vote = Counter(decision).most_common(1)[0][0]          # vote process: return the highest frequency element\n",
        "      if vote != y[i]:\n",
        "        a += 1\n",
        "    error = a/num_data\n",
        "    return error\n",
        "\n",
        "error_list_train, error_list_test, total_iteration = (pegasos_multiclass(data_2, value_2, data_4, value_4))\n",
        "print_error(error_list_train, error_list_test, total_iteration)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR9CJTw3Tk2O"
      },
      "source": [
        "4. Categorize Multiple Class Based on CNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gBD0P3AWDvK"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\t# 各种层类型的实现\n",
        "import torch.nn.functional as F\t# 各中层函数的实现，与层类型对应，如：卷积函数、池化函数、归一化函数等等\n",
        "import torch.optim as optim\t# 实现各种优化算法的包\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=(1,1))\n",
        "        self.conv2 = nn.Conv2d(16, 256, kernel_size=3, padding=(1,1)) # output channel: 16 or 256?\n",
        "        self.fc1 = nn.Linear(256*26*26, 64) #这里26用的是图片在conv2出来后的维度\n",
        "        self.fc2 = nn.Linear(64, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        tmp = self.conv1(x)\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=2, stride=1, padding=0)\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2, stride=1, padding=0)\n",
        "        x = x.view(x.size(0), -1) #这里转置后，row就是batch的维度，后面的都是data的维度和channel的维度 回头查一下size(0)代表的数值\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, criterion): # 还可添加loss_func等参数\n",
        "  model.train() # 必备，将模型设置为训练模式\n",
        "  train_loss = 0\n",
        "  for batch_idx, (data, target) in enumerate(train_loader): # 从数据加载器迭代一个batch的数据\n",
        "      data, target = data.to(device), target.to(device) # 将数据存储CPU或者GPU\n",
        "      optimizer.zero_grad() # 清除所有优化的梯度\n",
        "      # print((data))\n",
        "      output = model(data)  # 喂入数据并前向传播获取输出\n",
        "      loss = criterion(output, target)\n",
        "      # loss = F.nll_loss(output, target) # 调用损失函数计算损失\n",
        "      loss.backward() # 反向传\n",
        "      optimizer.step() # 更新参数\n",
        "      train_loss += loss.item()\n",
        "  train_loss /= len(train_loader.dataset)\n",
        "  print(\"train_loss: \", train_loss)\n",
        "\n",
        "def test(model, device, test_loader, criterion):\n",
        "  model.eval() # 必备，将模型设置为评估模式\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  print(\"Enter Test Case\")\n",
        "  with torch.no_grad(): # 禁用梯度计算\n",
        "      for data, target in test_loader: # 从数据加载器迭代一个batch的数据\n",
        "          data, target = data.to(device), target.to(device) \n",
        "          output = model(data)\n",
        "          test_loss += criterion(output, target).item()\n",
        "          # test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "          pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item() # 统计预测正确个数\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('Test Loss: ', test_loss)\n",
        "  # print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "  #     test_loss, correct, len(test_loader.dataset),\n",
        "  #     100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "def defineDataLoader(trainDataSet, trainDataLabel, testDataSet, testDataLabel):\n",
        "  \"\"\"\n",
        "  return the dataloader based on dataset and it's label\n",
        "  \"\"\"\n",
        "  trainset = MyTrainData(trainDataSet, trainDataLabel)\n",
        "  testset =  MyTrainData(testDataSet, testDataLabel)\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size = 10 )\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size = 10)\n",
        "  return trainloader, testloader\n",
        "\n",
        "\n",
        "class MyTrainData(torch.utils.data.Dataset):   #需要繼承data.Dataset\n",
        "  def __init__(self, data, label, transform=None, train=True): #初始化文件路進或文件名\n",
        "    self.train = train \n",
        "    self.data = data\n",
        "    tmp = self.data.astype(np.float32)\n",
        "    tmp = (tmp - tmp.min()) / (tmp.max() - tmp.min())\n",
        "    self.data = torch.from_numpy(tmp).float()\n",
        "    self.data = self.data.unsqueeze(-3)\n",
        "    self.label = torch.tensor(label).long() # Target is supposed to be long type\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.train:\n",
        "      img = self.data[idx,:,:,:]\n",
        "      gt = self.label[idx]\n",
        "      return img, gt\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data[:,:,0,0])\n",
        "\n",
        "def target_modif(target):\n",
        "  for i in range(len(target)):\n",
        "    if target[i] == 2:\n",
        "      target[i] = 0\n",
        "    elif target[i] == 5:\n",
        "      target[i] = 1\n",
        "    else:\n",
        "      target[i] = 2\n",
        "  return target\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # define some parameter here\n",
        "  TotalIteration = 1000\n",
        "  LearningRate = 0.01\n",
        "  trainError = []\n",
        "  testError  = []\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "  # Make instance/object for all the component\n",
        "  Netmodel = Net()\n",
        "  Netmodel.to(device)\n",
        "  value_2 = target_modif(value_2)\n",
        "  value_4 = target_modif(value_4)\n",
        "  trainloader, testloader = defineDataLoader(data_2, value_2, data_4, value_4) #need to input the data\n",
        "  optimizer = optim.SGD(Netmodel.parameters(), lr=LearningRate)\n",
        "  # device = torch.device(\"cpu\")\n",
        "\n",
        "  # Train and Test\n",
        "  for i in range(TotalIteration):\n",
        "    print(i)\n",
        "    train(Netmodel, device, trainloader, optimizer, i, criterion)\n",
        "    test(Netmodel, device, testloader, criterion)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}